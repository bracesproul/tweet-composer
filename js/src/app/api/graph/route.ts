import { VercelMemoryStore } from "@/stores/vercel";
import {
  Annotation,
  END,
  MessagesAnnotation,
  SharedValue,
  START,
  StateGraph,
} from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";
import { z } from "zod";
import { createClient } from "@vercel/kv";
import { NextRequest, NextResponse } from "next/server";
import { CoreMessage } from "@assistant-ui/react";

const GraphAnnotation = Annotation.Root({
  ...MessagesAnnotation.spec,
  userRules: SharedValue.on("userId"),
  /**
   * Whether or not the user has accepted the text generated by the AI.
   * If this is true, the graph will route to a node which generates rules.
   */
  hasAcceptedText: Annotation<boolean>(),
});

const DEFAULT_RULES_STRING = "*no rules have been set yet*";

const SYSTEM_PROMPT = `You are a helpful assistant tasked with thoughtfully fulfilling the requests of the user.
User defined rules:
{userRules}`;

const callModel = async (state: typeof GraphAnnotation.State) => {
  const model = new ChatOpenAI({
    model: "gpt-4o-mini",
    temperature: 0,
  });

  let rules = DEFAULT_RULES_STRING;
  if (state.userRules && state.userRules.rules?.length) {
    rules = `- ${state.userRules.rules.join("\n - ")}`;
  }

  const systemPrompt = SYSTEM_PROMPT.replace("{userRules}", rules);

  const response = await model.invoke([
    {
      role: "system",
      content: systemPrompt,
    },
    ...state.messages,
  ]);

  return { messages: [response] };
};

/**
 * This node generates insights based on the changes, or followup messages
 * that have been made by the user. It does the following:
 * 1. Sets a system message describing the task, and existing user rules, and how messages in the history can be formatted. (e.g an AI Message, followed by a human message that is prefixed with "REVISION").
 * 2. Passes the entire history to the LLM
 * 3. Uses `withStructuredOutput` to generate structured rules based on conversation or revisions.
 * 4. Updates the `userRules` shared value with the new rules.
 *
 * The LLM will always re-generate the entire rules list, so it is important to pass the entire history to the model.
 * @param state The current state of the graph
 */
const generateInsights = async (state: typeof GraphAnnotation.State) => {
  let systemPrompt = `You are a helpful assistant, tasked with generating rules based on insights you've gathered from the following conversation.
This conversation contains back and fourth between an AI assistant, and a user who is using the assistant to generate text for things such as writing tweets.
User messages which are prefixed with "REVISION" contain the entire revised text the user made to the assistant message directly before in the conversation.
There also may be additional back and fourth between the user and the assistant which you should consider when generating rules.

In your response, include every single rule, including those which already existed. You should only ever exclude rule(s) if the user has explicitly stated something which contradicts a previous rule.
The user and assistant may have had conversations before which you do not have access to, so be careful when removing rules.

The user has defined the following rules:
{userRules}`;

  let rules = DEFAULT_RULES_STRING;
  if (state.userRules && state.userRules.rules?.length) {
    rules = `- ${state.userRules.rules.join("\n - ")}`;
  }

  systemPrompt = systemPrompt.replace("{userRules}", rules);

  const userRulesSchema = z.object({
    rules: z
      .array(z.string())
      .describe("The rules which you have inferred from the conversation."),
  });

  const modelWithStructuredOutput = new ChatOpenAI({
    model: "gpt-4o",
    temperature: 0,
  }).withStructuredOutput(userRulesSchema, { name: "userRules" });

  const result = await modelWithStructuredOutput.invoke([
    systemPrompt,
    ...state.messages,
  ]);

  return {
    userRules: {
      rules: result.rules,
    },
    userAcceptedText: false,
  };
};

/**
 * Conditional edge which is always called first. This edge
 * determines whether or not revisions have been made, and if so,
 * generate insights to then set under user rules.
 * @param {typeof GraphAnnotation.State} state The current state of the graph
 */
const shouldGenerateInsights = (state: typeof GraphAnnotation.State) => {
  const { userAcceptedText } = state;
  if (userAcceptedText) {
    // Greater than three means there was at least one followup message after the original AI Message.
    return "generateInsights";
  }
  return "callModel";
};

const vercelKvClient = () => {
  if (!process.env.KV_REST_API_TOKEN || !process.env.KV_REST_API_URL) {
    throw new Error("Missing Vercel token or URL environment");
  }

  return createClient({
    token: process.env.KV_REST_API_TOKEN,
    url: process.env.KV_REST_API_URL,
  });
};

function buildGraph() {
  const workflow = new StateGraph(GraphAnnotation)
    .addNode("callModel", callModel)
    .addNode("generateInsights", generateInsights)
    // Always start by checking whether or not to generate insights
    .addConditionalEdges(START, shouldGenerateInsights)
    // No further action by the graph is necessary after either
    // generating a response via `callModel`, or rules via `generateInsights`.
    .addEdge("callModel", END)
    .addEdge("generateInsights", END);

  const store = new VercelMemoryStore({
    client: vercelKvClient(),
  });

  return workflow.compile({
    store,
  });
}

const mapMessagesToOpenAI = (
  messages: CoreMessage[]
): { role: string; content: string }[] => {
  return messages.map(({ role, content }) => {
    if (!content) {
      console.log("wtf no content?", messages);
      throw new Error("Invalid message content");
    }
    if ("text" in content[0]) {
      return {
        role,
        content: content[0].text,
      };
    } else {
      throw new Error("Invalid message content");
    }
  });
};

export async function POST(req: NextRequest) {
  const reqJson = await req.json();
  const { messages, userId, hasAcceptedText } = reqJson;
  console.log("userId", userId, "hasAcceptedText", hasAcceptedText);
  const messagesMapped = mapMessagesToOpenAI(messages);
  const graph = buildGraph();

  const config = { configurable: { userId: "123" }, version: "v2" as const };
  const stream = graph.streamEvents(
    { messages: messagesMapped, hasAcceptedText },
    config
  );

  const encoder = new TextEncoder();

  const readableStream = new ReadableStream({
    async start(controller) {
      try {
        for await (const event of stream) {
          controller.enqueue(encoder.encode(JSON.stringify(event) + "\n"));
        }
      } finally {
        controller.close();
      }
    },
  });

  return new NextResponse(readableStream, {
    headers: {
      "Content-Type": "application/json",
      "Transfer-Encoding": "chunked",
    },
  });
}
